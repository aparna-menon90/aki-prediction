{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37e29cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b7beb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "hosp='W:\\\\Main_project\\\\mimic-iv-2.2\\\\mimic-iv-2.2\\\\hosp\\\\'\n",
    "icu='W:\\\\Main_project\\\\mimic-iv-2.2\\\\mimic-iv-2.2\\\\icu\\\\'\n",
    "base='W:\\\\Main_project\\\\base_files\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4752d0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_adm=pd.read_csv(f'{base}pat_adm_latest_encounter_filtered.csv',index_col=0)\n",
    "subject_ids=pat_adm['subject_id'].unique()  # Example set of subject_ids\n",
    "file_path=f'{hosp}\\\\labevents.csv'  # Provide the path to your CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd8c49f",
   "metadata": {},
   "source": [
    "## Creatinine - Blood (50912)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2864b9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_scr_in_chunks_and_save(file_path, subject_ids, item_id=50912, chunksize=100000000):\n",
    "    # Create a directory to store filtered chunks\n",
    "    save_dir = os.path.dirname(file_path)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Iterate over the file in chunks\n",
    "    chunk_index = 1\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "        # Filter rows based on subject_ids\n",
    "        filtered_chunk = chunk[chunk['subject_id'].isin(subject_ids)]\n",
    "        \n",
    "        # Filter rows based on item_id\n",
    "        filtered_chunk = filtered_chunk[filtered_chunk['itemid'] == item_id]\n",
    "        \n",
    "        # Save the filtered chunk to a separate CSV file\n",
    "        save_path = os.path.join(save_dir, f'chunk{chunk_index}_scr.csv')\n",
    "        filtered_chunk.to_csv(save_path, index=False)\n",
    "        \n",
    "        chunk_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0ec958",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter_scr_in_chunks_and_save(file_path, subject_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aac3c7",
   "metadata": {},
   "source": [
    "## Albumin(50862)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b26d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_albumin_in_chunks_and_save(file_path, subject_ids, item_id=50862, chunksize=1000000):\n",
    "    # Create a directory to store filtered chunks\n",
    "    save_dir = os.path.dirname('W:\\\\Main_project\\\\chunks\\\\')\n",
    "    print(save_dir)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Iterate over the file in chunks\n",
    "    chunk_index = 1\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "        # Filter rows based on subject_ids\n",
    "        filtered_chunk = chunk[chunk['subject_id'].isin(subject_ids)]\n",
    "        \n",
    "        # Filter rows based on item_id\n",
    "        filtered_chunk = filtered_chunk[filtered_chunk['itemid'] == item_id]\n",
    "        \n",
    "        # Save the filtered chunk to a separate CSV file\n",
    "        save_path = os.path.join(save_dir, f'chunk{chunk_index}_albumin.csv')\n",
    "        filtered_chunk.to_csv(save_path, index=False)\n",
    "        \n",
    "        chunk_index += 1\n",
    "    last_index = chunk_index - 1\n",
    "    print(f\"Last index of the file read: {last_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42666c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_albumin_in_chunks_and_save(file_path,subject_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90810dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'W:\\\\Main_project\\\\chunks\\\\'\n",
    "# List to hold individual DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith('chunk') and filename.endswith('_albumin.csv'):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        # Read the CSV file into a DataFrame and append it to the list\n",
    "        dfs.append(pd.read_csv(filepath))\n",
    "\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "scr_df = pd.concat(dfs, ignore_index=True)\n",
    "scr_df.to_csv(f'{hosp}albumin_lab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf5862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del scr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00f0877",
   "metadata": {},
   "source": [
    "## ALT (50861)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa48039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_alt_in_chunks_and_save(file_path, subject_ids, item_id=50861, chunksize=1000000):\n",
    "    # Create a directory to store filtered chunks\n",
    "    save_dir = os.path.dirname('W:\\\\Main_project\\\\chunks\\\\')\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Iterate over the file in chunks\n",
    "    chunk_index = 1\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "        # Filter rows based on subject_ids\n",
    "        filtered_chunk = chunk[chunk['subject_id'].isin(subject_ids)]\n",
    "        \n",
    "        # Filter rows based on item_id\n",
    "        filtered_chunk = filtered_chunk[filtered_chunk['itemid'] == item_id]\n",
    "        \n",
    "        # Save the filtered chunk to a separate CSV file\n",
    "        save_path = os.path.join(save_dir, f'chunk{chunk_index}_alt.csv')\n",
    "        filtered_chunk.to_csv(save_path, index=False)\n",
    "        \n",
    "        chunk_index += 1\n",
    "    last_index = chunk_index - 1\n",
    "    print(f\"Last index of the file read: {last_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce40c93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_alt_in_chunks_and_save(file_path,subject_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cad943",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'W:\\\\Main_project\\\\chunks\\\\'\n",
    "# List to hold individual DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith('chunk') and filename.endswith('_alt.csv'):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        # Read the CSV file into a DataFrame and append it to the list\n",
    "        dfs.append(pd.read_csv(filepath))\n",
    "\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "scr_df = pd.concat(dfs, ignore_index=True)\n",
    "scr_df.to_csv(f'{hosp}alt_lab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f49d388",
   "metadata": {},
   "outputs": [],
   "source": [
    "scr_df['subject_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04134f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del scr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae7e8a3",
   "metadata": {},
   "source": [
    "## AST(50878)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2d9d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_ids=pat_adm['subject_id'].unique().astype(str)\n",
    "def filter_ast_in_chunks_and_save(file_path, subject_ids, item_id='50878', chunksize=1000000):\n",
    "    # Create a directory to store filtered chunks\n",
    "    save_dir = os.path.dirname(file_path)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Iterate over the file in chunks\n",
    "    chunk_index = 1\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize,dtype=str,on_bad_lines='skip'):\n",
    "        # Filter rows based on subject_ids\n",
    "        filtered_chunk = chunk[chunk['subject_id'].isin(subject_ids)]\n",
    "        \n",
    "        # Filter rows based on item_id\n",
    "        filtered_chunk = filtered_chunk[filtered_chunk['itemid'] == item_id]\n",
    "        \n",
    "        # Save the filtered chunk to a separate CSV file\n",
    "        save_path = os.path.join(save_dir, f'chunk{chunk_index}_ast.csv')\n",
    "        filtered_chunk.to_csv(save_path, index=False)\n",
    "        \n",
    "        chunk_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06ea443",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_ast_in_chunks_and_save(file_path,subject_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044acd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'W:\\\\Main_project\\\\mimic-iv-2.2\\\\mimic-iv-2.2\\\\hosp'\n",
    "# List to hold individual DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith('chunk') and filename.endswith('_ast.csv'):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        # Read the CSV file into a DataFrame and append it to the list\n",
    "        dfs.append(pd.read_csv(filepath))\n",
    "\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "scr_df = pd.concat(dfs, ignore_index=True)\n",
    "scr_df.to_csv(f'{hosp}ast_lab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b80002",
   "metadata": {},
   "outputs": [],
   "source": [
    "scr_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d8e9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del scr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adc420a",
   "metadata": {},
   "source": [
    "## Ammonia(50866)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb859ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ammonia_in_chunks_and_save(file_path, subject_ids, item_id=50866, chunksize=1000000):\n",
    "    # Create a directory to store filtered chunks\n",
    "    save_dir = os.path.dirname('W:\\\\Main_project\\\\chunks\\\\')\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Iterate over the file in chunks\n",
    "    chunk_index = 1\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "        # Filter rows based on subject_ids\n",
    "        filtered_chunk = chunk[chunk['subject_id'].isin(subject_ids)]\n",
    "        \n",
    "        # Filter rows based on item_id\n",
    "        filtered_chunk = filtered_chunk[filtered_chunk['itemid'] == item_id]\n",
    "        \n",
    "        # Save the filtered chunk to a separate CSV file\n",
    "        save_path = os.path.join(save_dir, f'chunk{chunk_index}_ammonia.csv')\n",
    "        filtered_chunk.to_csv(save_path, index=False)\n",
    "        \n",
    "        chunk_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1700b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_ammonia_in_chunks_and_save(file_path,subject_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaac46e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'W:\\\\Main_project\\\\mimic-iv-2.2\\\\mimic-iv-2.2\\\\hosp'\n",
    "# List to hold individual DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith('chunk') and filename.endswith('_ammonia.csv'):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        # Read the CSV file into a DataFrame and append it to the list\n",
    "        dfs.append(pd.read_csv(filepath))\n",
    "\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "scr_df = pd.concat(dfs, ignore_index=True)\n",
    "scr_df.to_csv(f'{hosp}ammonia_lab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ad18fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "del scr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be916c1",
   "metadata": {},
   "source": [
    "## Blood Bilirubin(50885)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3238dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_ids=pat_adm['subject_id'].unique().astype(str)\n",
    "def filter_bili_in_chunks_and_save(file_path, subject_ids, item_id='50885', chunksize=1000000):\n",
    "    # Create a directory to store filtered chunks\n",
    "    save_dir = os.path.dirname('W:\\\\Main_project\\\\chunks\\\\')\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Iterate over the file in chunks\n",
    "    chunk_index = 1\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize,on_bad_lines='skip',dtype=str):\n",
    "        # Filter rows based on subject_ids\n",
    "        filtered_chunk = chunk[chunk['subject_id'].isin(subject_ids)]\n",
    "        \n",
    "        # Filter rows based on item_id\n",
    "        filtered_chunk = filtered_chunk[filtered_chunk['itemid'] == item_id]\n",
    "        \n",
    "        # Save the filtered chunk to a separate CSV file\n",
    "        save_path = os.path.join(save_dir, f'chunk{chunk_index}_bili.csv')\n",
    "        filtered_chunk.to_csv(save_path, index=False)\n",
    "        \n",
    "        chunk_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a436826",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_bili_in_chunks_and_save(file_path, subject_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eed195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'W:\\\\Main_project\\\\chunks\\\\'\n",
    "# List to hold individual DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith('chunk') and filename.endswith('_bili.csv'):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        # Read the CSV file into a DataFrame and append it to the list\n",
    "        dfs.append(pd.read_csv(filepath))\n",
    "\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "scr_df = pd.concat(dfs, ignore_index=True)\n",
    "scr_df.to_csv(f'{hosp}bilirubin_lab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8ff49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scr_df['subject_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57c5972",
   "metadata": {},
   "source": [
    "## BUN(51006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ec8273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no data found\n",
    "subject_ids=pat_adm['subject_id'].unique().astype(str)\n",
    "def filter_bun_in_chunks_and_save(file_path, subject_ids, item_id='51006', chunksize=1000000):\n",
    "    # Create a directory to store filtered chunks\n",
    "    save_dir = os.path.dirname('W:\\\\Main_project\\\\chunks\\\\')\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Iterate over the file in chunks\n",
    "    chunk_index = 1\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize,on_bad_lines='skip',dtype=str):\n",
    "        # Filter rows based on subject_ids\n",
    "        filtered_chunk = chunk[chunk['subject_id'].isin(subject_ids)]\n",
    "        \n",
    "        # Filter rows based on item_id\n",
    "        filtered_chunk = filtered_chunk[filtered_chunk['itemid'] == item_id]\n",
    "        \n",
    "        # Save the filtered chunk to a separate CSV file\n",
    "        save_path = os.path.join(save_dir, f'chunk{chunk_index}_bun.csv')\n",
    "        filtered_chunk.to_csv(save_path, index=False)\n",
    "        \n",
    "        chunk_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b162d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_bun_in_chunks_and_save(file_path, subject_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563e9ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'W:\\\\Main_project\\\\chunks\\\\'\n",
    "# List to hold individual DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith('chunk') and filename.endswith('_bun.csv'):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        # Read the CSV file into a DataFrame and append it to the list\n",
    "        dfs.append(pd.read_csv(filepath))\n",
    "\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "scr_df = pd.concat(dfs, ignore_index=True)\n",
    "scr_df.to_csv(f'{hosp}bun_lab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06419940",
   "metadata": {},
   "outputs": [],
   "source": [
    "scr_df['subject_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff09dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "del scr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c04770a",
   "metadata": {},
   "source": [
    "## Calcium(50893)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9345f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_calcium_in_chunks_and_save(file_path, subject_ids, item_id=50893, chunksize=1000000):\n",
    "    # Create a directory to store filtered chunks\n",
    "    save_dir = os.path.dirname(file_path)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Iterate over the file in chunks\n",
    "    chunk_index = 1\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "        # Filter rows based on subject_ids\n",
    "        filtered_chunk = chunk[chunk['subject_id'].isin(subject_ids)]\n",
    "        \n",
    "        # Filter rows based on item_id\n",
    "        filtered_chunk = filtered_chunk[filtered_chunk['itemid'] == item_id]\n",
    "        \n",
    "        # Save the filtered chunk to a separate CSV file\n",
    "        save_path = os.path.join(save_dir, f'chunk{chunk_index}_calcium.csv')\n",
    "        filtered_chunk.to_csv(save_path, index=False)\n",
    "        \n",
    "        chunk_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "099c2db8",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m filter_calcium_in_chunks_and_save(file_path, subject_ids)\n",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m, in \u001b[0;36mfilter_calcium_in_chunks_and_save\u001b[1;34m(file_path, subject_ids, item_id, chunksize)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Iterate over the file in chunks\u001b[39;00m\n\u001b[0;32m      8\u001b[0m chunk_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, chunksize\u001b[38;5;241m=\u001b[39mchunksize):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Filter rows based on subject_ids\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     filtered_chunk \u001b[38;5;241m=\u001b[39m chunk[chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(subject_ids)]\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Filter rows based on item_id\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1624\u001b[0m, in \u001b[0;36mTextFileReader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1622\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m   1623\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1624\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_chunk()\n\u001b[0;32m   1625\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   1626\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1733\u001b[0m, in \u001b[0;36mTextFileReader.get_chunk\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1731\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m   1732\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow)\n\u001b[1;32m-> 1733\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nrows\u001b[38;5;241m=\u001b[39msize)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1697\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1699\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1700\u001b[0m     (\n\u001b[0;32m   1701\u001b[0m         index,\n\u001b[0;32m   1702\u001b[0m         columns,\n\u001b[0;32m   1703\u001b[0m         col_dict,\n\u001b[1;32m-> 1704\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m         nrows\n\u001b[0;32m   1706\u001b[0m     )\n\u001b[0;32m   1707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:826\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:875\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:2021\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied"
     ]
    }
   ],
   "source": [
    "filter_calcium_in_chunks_and_save(file_path, subject_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3d1a04",
   "metadata": {},
   "source": [
    "## CK-MB(50908)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa41e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_ids=pat_adm['subject_id'].unique().astype(str)\n",
    "def filter_ckmb_in_chunks_and_save(file_path, subject_ids, item_id='50908', chunksize=1000000):\n",
    "    # Create a directory to store filtered chunks\n",
    "    save_dir = os.path.dirname(file_path)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Iterate over the file in chunks\n",
    "    chunk_index = 1\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize,dtype=str,on_bad_lines='skip'):\n",
    "        # Filter rows based on subject_ids\n",
    "        filtered_chunk = chunk[chunk['subject_id'].isin(subject_ids)]\n",
    "        \n",
    "        # Filter rows based on item_id\n",
    "        filtered_chunk = filtered_chunk[filtered_chunk['itemid'] == item_id]\n",
    "        \n",
    "        # Save the filtered chunk to a separate CSV file\n",
    "        save_path = os.path.join(save_dir, f'chunk{chunk_index}_ckmb.csv')\n",
    "        filtered_chunk.to_csv(save_path, index=False)\n",
    "        \n",
    "        chunk_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb7a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_ckmb_in_chunks_and_save(file_path, subject_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3010c42",
   "metadata": {},
   "source": [
    "## CK(50910)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bda61da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ck_in_chunks_and_save(file_path, subject_ids, item_id=50910, chunksize=1000000):\n",
    "    # Create a directory to store filtered chunks\n",
    "    save_dir = os.path.dirname(file_path)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Iterate over the file in chunks\n",
    "    chunk_index = 1\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize,on_bad_lines='skip'):\n",
    "        # Filter rows based on subject_ids\n",
    "        filtered_chunk = chunk[chunk['subject_id'].isin(subject_ids)]\n",
    "        \n",
    "        # Filter rows based on item_id\n",
    "        filtered_chunk = filtered_chunk[filtered_chunk['itemid'] == item_id]\n",
    "        \n",
    "        # Save the filtered chunk to a separate CSV file\n",
    "        save_path = os.path.join(save_dir, f'chunk{chunk_index}_ck.csv')\n",
    "        filtered_chunk.to_csv(save_path, index=False)\n",
    "        \n",
    "        chunk_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a27bc075",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_ck_in_chunks_and_save(file_path, subject_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee1ea170",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'W:\\\\Main_project\\\\mimic-iv-2.2\\\\mimic-iv-2.2\\\\hosp'\n",
    "# List to hold individual DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith('chunk') and filename.endswith('_ck.csv'):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        # Read the CSV file into a DataFrame and append it to the list\n",
    "        dfs.append(pd.read_csv(filepath))\n",
    "\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "scr_df = pd.concat(dfs, ignore_index=True)\n",
    "scr_df.to_csv(f'{hosp}ck_lab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b783d90f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61719, 16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scr_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f1e766",
   "metadata": {},
   "source": [
    "## Glucose(50931)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f453b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_glucose_in_chunks_and_save(file_path, subject_ids, item_id=50931, chunksize=1000000):\n",
    "    # Create a directory to store filtered chunks\n",
    "    save_dir = os.path.dirname(file_path)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Iterate over the file in chunks\n",
    "    chunk_index = 1\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize,on_bad_lines = 'skip'):\n",
    "        # Filter rows based on subject_ids\n",
    "        filtered_chunk = chunk[chunk['subject_id'].isin(subject_ids)]\n",
    "        \n",
    "        # Filter rows based on item_id\n",
    "        filtered_chunk = filtered_chunk[filtered_chunk['itemid'] == item_id]\n",
    "        \n",
    "        # Save the filtered chunk to a separate CSV file\n",
    "        save_path = os.path.join(save_dir, f'chunk{chunk_index}_glucose.csv')\n",
    "        filtered_chunk.to_csv(save_path, index=False)\n",
    "        \n",
    "        chunk_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc8e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_glucose_in_chunks_and_save(file_path, subject_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3faf8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to hold individual DataFrames\n",
    "directory = 'W:\\\\Main_project\\\\mimic-iv-2.2\\\\mimic-iv-2.2\\\\hosp'\n",
    "dfs = []\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith('chunk') and filename.endswith('_glucose.csv'):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        # Read the CSV file into a DataFrame and append it to the list\n",
    "        dfs.append(pd.read_csv(filepath))\n",
    "\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "scr_df = pd.concat(dfs, ignore_index=True)\n",
    "scr_df.to_csv(f'{hosp}glucose_lab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a480cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scr_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf7f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "del scr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0336d3f7",
   "metadata": {},
   "source": [
    "## Lipase(50956)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff7df77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_lipase_in_chunks_and_save(file_path, subject_ids, item_id=50956, chunksize=100000):\n",
    "    # Create a directory to store filtered chunks\n",
    "    save_dir = os.path.dirname(file_path)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Iterate over the file in chunks\n",
    "    chunk_index = 1\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize,low_memory=False):\n",
    "        # Filter rows based on subject_ids\n",
    "        filtered_chunk = chunk[chunk['subject_id'].isin(subject_ids)]\n",
    "        \n",
    "        # Filter rows based on item_id\n",
    "        filtered_chunk = filtered_chunk[filtered_chunk['itemid'] == item_id]\n",
    "        \n",
    "        # Save the filtered chunk to a separate CSV file\n",
    "        save_path = os.path.join(save_dir, f'chunk{chunk_index}_lipase.csv')\n",
    "        filtered_chunk.to_csv(save_path, index=False)\n",
    "        \n",
    "        chunk_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3e43aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_lipase_in_chunks_and_save(file_path, subject_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450988d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'W:\\\\Main_project\\\\mimic-iv-2.2\\\\mimic-iv-2.2\\\\hosp'\n",
    "# List to hold individual DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith('chunk') and filename.endswith('_lipase.csv'):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        # Read the CSV file into a DataFrame and append it to the list\n",
    "        dfs.append(pd.read_csv(filepath))\n",
    "\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "scr_df = pd.concat(dfs, ignore_index=True)\n",
    "scr_df.to_csv(f'{hosp}lipase_lab.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8140c35",
   "metadata": {},
   "source": [
    "## Platelets(51265)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0b885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_platelet_in_chunks_and_save(file_path, subject_ids, item_id=51265, chunksize=1000000):\n",
    "    # Create a directory to store filtered chunks\n",
    "    save_dir = os.path.dirname(file_path)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Iterate over the file in chunks\n",
    "    chunk_index = 1\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "        # Filter rows based on subject_ids\n",
    "        filtered_chunk = chunk[chunk['subject_id'].isin(subject_ids)]\n",
    "        \n",
    "        # Filter rows based on item_id\n",
    "        filtered_chunk = filtered_chunk[filtered_chunk['itemid'] == item_id]\n",
    "        \n",
    "        # Save the filtered chunk to a separate CSV file\n",
    "        save_path = os.path.join(save_dir, f'chunk{chunk_index}_platelet.csv')\n",
    "        filtered_chunk.to_csv(save_path, index=False)\n",
    "        \n",
    "        chunk_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f7dd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_platelet_in_chunks_and_save(file_path, subject_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b85045",
   "metadata": {},
   "source": [
    "## Troponin(51003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be932d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_troponin_in_chunks_and_save(file_path, subject_ids, item_id=51003, chunksize=1000000):\n",
    "    # Create a directory to store filtered chunks\n",
    "    save_dir = os.path.dirname(file_path)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Iterate over the file in chunks\n",
    "    chunk_index = 1\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "        # Filter rows based on subject_ids\n",
    "        filtered_chunk = chunk[chunk['subject_id'].isin(subject_ids)]\n",
    "        \n",
    "        # Filter rows based on item_id\n",
    "        filtered_chunk = filtered_chunk[filtered_chunk['itemid'] == item_id]\n",
    "        \n",
    "        # Save the filtered chunk to a separate CSV file\n",
    "        save_path = os.path.join(save_dir, f'chunk{chunk_index}_troponin.csv')\n",
    "        filtered_chunk.to_csv(save_path, index=False)\n",
    "        \n",
    "        chunk_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619a7265",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_troponin_in_chunks_and_save(file_path, subject_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd8ff90",
   "metadata": {},
   "source": [
    "## WBC(51301)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33812a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_wbc_in_chunks_and_save(file_path, subject_ids, item_id=51301, chunksize=1000000):\n",
    "    # Create a directory to store filtered chunks\n",
    "    save_dir = os.path.dirname(file_path)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Iterate over the file in chunks\n",
    "    chunk_index = 1\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "        # Filter rows based on subject_ids\n",
    "        filtered_chunk = chunk[chunk['subject_id'].isin(subject_ids)]\n",
    "        \n",
    "        # Filter rows based on item_id\n",
    "        filtered_chunk = filtered_chunk[filtered_chunk['itemid'] == item_id]\n",
    "        \n",
    "        # Save the filtered chunk to a separate CSV file\n",
    "        save_path = os.path.join(save_dir, f'chunk{chunk_index}_wbc.csv')\n",
    "        filtered_chunk.to_csv(save_path, index=False)\n",
    "        \n",
    "        chunk_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59763d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_wbc_in_chunks_and_save(file_path, subject_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
