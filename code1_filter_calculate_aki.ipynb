{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2c6f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fe523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE FILEPATH\n",
    "hosp='W:\\\\Main_project\\\\mimic-iv-2.2\\\\mimic-iv-2.2\\\\hosp\\\\'\n",
    "icu='W:\\\\Main_project\\\\mimic-iv-2.2\\\\mimic-iv-2.2\\\\icu\\\\'\n",
    "base='W:\\\\Main_project\\\\base_files\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560b10b4",
   "metadata": {},
   "source": [
    "## Read main files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a76fc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Read files\n",
    "#Files required\n",
    "#patients,admissions,procedures,labevents(subset),diagnoses,chartevents,icustays,d_diagnoses,d_proc,d_labitems\n",
    "#hosp\n",
    "\n",
    "patients=pd.read_csv(f'{hosp}patients.csv')\n",
    "admissions=pd.read_csv(f'{hosp}admissions.csv')\n",
    "#omr=pd.read_csv(f'{hosp}omr.csv')\n",
    "procedures=pd.read_csv(f'{hosp}procedures_icd.csv')\n",
    "d_procedures=pd.read_csv(f'{hosp}d_icd_procedures.csv')\n",
    "diagnoses=pd.read_csv(f'{hosp}diagnoses_icd.csv')\n",
    "d_diagnoses=pd.read_csv(f'{hosp}d_icd_diagnoses.csv')\n",
    "labitems=pd.read_csv(f'{hosp}d_labitems.csv')\n",
    "prescription=pd.read_csv(f'{hosp}prescriptions.csv')\n",
    "#icu folder\n",
    "icustays=pd.read_csv(f'{icu}icustays.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8823fbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions to read large files\n",
    "def read_large_file(file,path):\n",
    "    filename=pd.read_csv(f'{path}{file}',chunksize=10000)\n",
    "    chunk_list = []\n",
    "    for chunk in labs:    \n",
    "        chunk_list.append(chunk)\n",
    "        filename = pd.concat(chunk_list)\n",
    "    return filename\n",
    "\n",
    "#Function to get creatinine labs\n",
    "def filter_scr_in_chunks_and_save(file_path, subject_ids, item_id=50912, chunksize=100000000):\n",
    "    # Create a directory to store filtered chunks\n",
    "    save_dir = os.path.dirname(file_path)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Iterate over the file in chunks\n",
    "    chunk_index = 1\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "        # Filter rows based on subject_ids\n",
    "        filtered_chunk = chunk[chunk['subject_id'].isin(subject_ids)]\n",
    "        \n",
    "        # Filter rows based on item_id\n",
    "        filtered_chunk = filtered_chunk[filtered_chunk['itemid'] == item_id]\n",
    "        \n",
    "        # Save the filtered chunk to a separate CSV file\n",
    "        save_path = os.path.join(save_dir, f'chunk{chunk_index}_scr.csv')\n",
    "        filtered_chunk.to_csv(save_path, index=False)\n",
    "        \n",
    "        chunk_index += 1\n",
    "\n",
    "\n",
    "#Function to read only top rows to examine a file\n",
    "def read_top_nrows(file,path,n_rows):\n",
    "    filename=pd.read_csv(f'{path}{file}',nrows=n_rows)\n",
    "    return filename\n",
    "\n",
    "\n",
    "#####################\n",
    "\n",
    "def map_valuenum(admissions_df, values_df):\n",
    "    # Convert admittime and dischtime to datetime objects\n",
    "    admissions_df['admittime'] = pd.to_datetime(admissions_df['admittime'])\n",
    "    admissions_df['dischtime'] = pd.to_datetime(admissions_df['dischtime'])\n",
    "    \n",
    "    # Convert charttime to datetime object\n",
    "    values_df['charttime'] = pd.to_datetime(values_df['charttime'])\n",
    "    \n",
    "    # Initialize an empty list to store mapped values\n",
    "    mapped_values = []\n",
    "    \n",
    "    # Iterate through each row in admissions_df\n",
    "    for index, row in admissions_df.iterrows():\n",
    "        subject_id = row['subject_id']\n",
    "        admittime = row['admittime']\n",
    "        dischtime = row['dischtime']\n",
    "        \n",
    "        # Filter values_df for rows within the admission time window for the subject_id\n",
    "        filtered_values = values_df[(values_df['subject_id'] == subject_id) & \n",
    "                                    (values_df['charttime'] >= admittime) &\n",
    "                                    (values_df['charttime'] <= dischtime)]\n",
    "        \n",
    "        if not filtered_values.empty:\n",
    "            # Find the row with the earliest charttime\n",
    "            earliest_row = filtered_values.loc[filtered_values['charttime'].idxmin()]\n",
    "            \n",
    "            # Extract valuenum and charttime\n",
    "            valuenum = earliest_row['valuenum']\n",
    "            charttime = earliest_row['charttime']\n",
    "            \n",
    "            # Append the mapped values to the list\n",
    "            mapped_values.append({'subject_id': subject_id, \n",
    "                                  'admittime': admittime,\n",
    "                                  'dischtime': dischtime,\n",
    "                                  'earliest_charttime': charttime,\n",
    "                                  'earliest_valuenum': valuenum})\n",
    "    \n",
    "    # Create a DataFrame from the mapped values\n",
    "    mapped_df = pd.DataFrame(mapped_values)\n",
    "    \n",
    "    return mapped_df\n",
    "\n",
    "\n",
    "#Calculate egfr based on creatinine values\n",
    "\n",
    "def calculate_egfr(row):\n",
    "    if row['gender'] == 'F':\n",
    "        gender_factor = 0.742\n",
    "    else:\n",
    "        gender_factor = 1\n",
    "\n",
    "    if 'BLACK' in row['race'].upper():\n",
    "        race_factor = 1.212\n",
    "    else:\n",
    "        race_factor = 1\n",
    "\n",
    "    # Adding checks for zero division and null values\n",
    "    if row['earliest_creatinine'] is not None and row['age'] is not None \\\n",
    "            and row['earliest_creatinine'] > 0 and row['age'] > 0:\n",
    "        egfr = 175 * (row['earliest_creatinine'] ** -1.154) * (row['age'] ** -0.203) * gender_factor * race_factor\n",
    "        return egfr\n",
    "    else:\n",
    "        # Return a default value or handle the error as appropriate\n",
    "        return None  # Or any other value or action you prefer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db34f538",
   "metadata": {},
   "source": [
    "## Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85775b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No of patients\",patients['subject_id'].nunique())\n",
    "print(\"No of admissions\",admissions['hadm_id'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b6772b",
   "metadata": {},
   "source": [
    "## Get latest encounter for a patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3d8256",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first get the latest encounter for each patient and then filter\n",
    "pat_adm=pd.merge(patients,admissions,how='inner',on='subject_id')\n",
    "filter_df=pat_adm.groupby('subject_id')['admittime'].max().reset_index() #max admittime\n",
    "pat_adm1 = pd.merge(pat_adm, filter_df, on=['subject_id', 'admittime'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42c31af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pat_adm1[pat_adm1['subject_id'].isin([10000032,10000068,10000084])] #sanity check\n",
    "#pat_adm[pat_adm['subject_id'].isin([10000032,10000068,10000084])] #sanity check\n",
    "del filter_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83873b8",
   "metadata": {},
   "source": [
    "## Filter criteria -Age,hospital stay,dialysis,eGFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbefbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter criteria\n",
    "\n",
    "#Age\n",
    "\n",
    "pat_adm1['birth_year'] = pat_adm1['anchor_year'] - pat_adm1['anchor_age']\n",
    "pat_adm1['admittime'] = pd.to_datetime(pat_adm1['admittime'])\n",
    "pat_adm1['anchor_year'] = pd.to_datetime(pat_adm1['anchor_year'], format='%Y')\n",
    "pat_adm1['age'] = (pat_adm1['admittime'] - pd.to_datetime(pat_adm1['anchor_year'].dt.year, format='%Y')) / pd.Timedelta(days=365) + pat_adm1['anchor_age']\n",
    "pat_adm1['age'] = pat_adm1['age'].astype(int)\n",
    "\n",
    "print(\"Initial number of patients\",pat_adm['subject_id'].nunique()) # 180733 patients initally\n",
    "pat_adm1=pat_adm1[pat_adm1['age']>=18]\n",
    "print(\"After age conditions\",pat_adm1['subject_id'].nunique())\n",
    "\n",
    "\n",
    "#Hospital stay for >= 3 days\n",
    "\n",
    "pat_adm1['admittime'] = pd.to_datetime(pat_adm1['admittime'], format='%Y-%m-%d %H:%M:%S')\n",
    "pat_adm1['dischtime'] = pd.to_datetime(pat_adm1['dischtime'], format='%Y-%m-%d %H:%M:%S')\n",
    "pat_adm1['date_diff'] = (pat_adm1['dischtime'] - pat_adm1['admittime']).dt.days\n",
    "pat_adm1 = pat_adm1[pat_adm1['date_diff'] >= 3]\n",
    "print(\"After number of days of hospital stay conditions\",pat_adm1['subject_id'].nunique())\n",
    "\n",
    "\n",
    "\n",
    "#Did not undergo dialysis within 2 days of admission\n",
    "dialysis_codes=('5A1D00Z','5A1D60Z','5A1D70Z','5A1D80Z','5A1D90Z','3993','3995','5498')\n",
    "pat_proc= pd.merge(pat_adm1,procedures,on=['hadm_id','subject_id'],how='left')\n",
    "pat_proc['admittime'] = pd.to_datetime(pat_proc['admittime'], format='%Y-%m-%d')\n",
    "pat_proc['chartdate'] = pd.to_datetime(pat_proc['chartdate'], format='%Y-%m-%d')\n",
    "pat_proc['dialysis_diff']=(pat_proc['chartdate'] - pat_proc['admittime'].dt.floor('d')).dt.days\n",
    "pat_proc = pat_proc[pat_proc['dialysis_diff'] <= 2]\n",
    "#pat_proc contains patients who underwent dialysis within 2 days after admission ; hence remove these hadm_ids from pat_adm\n",
    "pat_adm1=pat_adm1[~pat_adm1['hadm_id'].isin(pat_proc['hadm_id'])]\n",
    "print(\"After dialysis within 2 days conditions\",pat_adm1['subject_id'].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b21791f",
   "metadata": {},
   "source": [
    "### Below code get the creatinine values to get egfr and then filter patients with egfr<=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b606f410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the creatinine labs file\n",
    "subject_ids=pat_adm['subject_id'].unique()  # Example set of subject_ids\n",
    "file_path=f'{hosp}\\\\labevents.csv'  # Provide the path to your CSV file\n",
    "filter_scr_in_chunks_and_save(file_path, subject_ids)\n",
    "\n",
    "\n",
    "#read the creatinine lab files into a df\n",
    "directory = f'{hosp}'\n",
    "dfs = []\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith('chunk') and filename.endswith('scr.csv'):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        # Read the CSV file into a DataFrame and append it to the list\n",
    "        dfs.append(pd.read_csv(filepath))\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "scr_df = pd.concat(dfs, ignore_index=True)\n",
    "scr_df.to_csv(f'{hosp}scr_labs.csv') # so that above code need not be run all the time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Map the scr values to patients\n",
    "scr_map = map_valuenum(pat_adm1, scr_df)\n",
    "scr_map.rename(columns={'earliest_valuenum': 'earliest_creatinine'}, inplace=True)\n",
    "#join to main df\n",
    "pat_adm1=pd.merge(pat_adm1,scr_map,on=['subject_id','admittime','dischtime'],how='left')\n",
    "\n",
    "\n",
    "### Caluclate egfr and remove patients with <=15\n",
    "pat_adm1['egfr'] = pat_adm1.apply(calculate_egfr, axis=1)\n",
    "pat_adm1 = pat_adm1[pat_adm1['egfr'] > 15]\n",
    "print(\"After removing patients with egfr<=15\",pat_adm1['subject_id'].nunique())\n",
    "#Save the file\n",
    "pat_adm1.to_csv(f'{hosp}pat_adm_latest_encounter_filtered.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42715c42",
   "metadata": {},
   "source": [
    "## Calculate if AKI occured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93eeefb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Criteria\n",
    "#An increase in serum creatinine by greater than or equal to 0.3 mg/dL within 48 hours; or\n",
    "#An increase in serum creatinine by greater than or equal to 1.5 times baseline, \n",
    "#which is known or presumed to have occurred within the prior 7 days; or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb8a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# are there any icustays which are not in admissions file? its not there, so we are good\n",
    "icustays[~icustays['hadm_id'].isin(admissions['hadm_id'])]['hadm_id'] # none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01496655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the patient and creatinine lab files, which was saved earlier from above code\n",
    "\n",
    "pat_adm1=pd.read_csv(f'{base}pat_adm_latest_encounter_filtered.csv',index_col=0)\n",
    "scr_df=pd.read_csv(f'{hosp}scr_labs.csv',index_col=0)\n",
    "#Take only required columns\n",
    "pat_adm1=pat_adm1[['subject_id', 'gender','hadm_id', 'admittime', 'dischtime','marital_status', 'race','age','earliest_creatinine', 'egfr']]\n",
    "scr_df=scr_df[['labevent_id', 'subject_id','itemid','charttime','valuenum','valueuom']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1618de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge df and convert the date columns\n",
    "df=pd.merge(pat_adm1,scr_df,how='left',on='subject_id')\n",
    "df=df[(df['charttime'] >= df['admittime']) & (df['charttime'] <= df['dischtime'])]\n",
    "df['admittime'] = pd.to_datetime(df['admittime'],  format='%Y-%m-%d %H:%M:%S')\n",
    "df['dischtime'] = pd.to_datetime(df['dischtime'],  format='%Y-%m-%d %H:%M:%S')\n",
    "df['charttime'] = pd.to_datetime(df['charttime'],  format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6345c2",
   "metadata": {},
   "source": [
    "### Function to check 48 hour increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aecf9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_increase(row, df):\n",
    "    current_time = row['charttime']\n",
    "    previous_measurements = df[(df['subject_id'] == row['subject_id']) & (df['charttime'] < current_time)]\n",
    "    window_start = current_time - timedelta(hours=48)\n",
    "    previous_measurements = previous_measurements[(previous_measurements['charttime'] >= window_start) & (previous_measurements['charttime'] < current_time)]\n",
    "    if len(previous_measurements) > 0:\n",
    "        min_value = previous_measurements['valuenum'].min()      \n",
    "        if round(row['valuenum'] - min_value,2) >= 0.3:\n",
    "            return 1, row['charttime'] #previous_measurements['charttime'].iloc[-1]\n",
    "    return 0, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88f4af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each row\n",
    "df[['increase_flag', 'start_time_of_increase']] = df.apply(lambda row: pd.Series(check_increase(row, df)), axis=1)\n",
    "#save the file\n",
    "df.to_csv(f'{base}48h_final.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876d47da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check counts\n",
    "print(df[df['increase_flag']==0]['subject_id'].nunique()) #27111\n",
    "print(df[df['increase_flag']==1]['subject_id'].nunique()) #5269"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9680f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test for one\n",
    "#df[df['subject_id']==19779215]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baba1d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get only one row per subject, with charttime\n",
    "df['start_time_of_increase'] = pd.to_datetime(df['start_time_of_increase'])\n",
    "\n",
    "# Group by 'subject_id' and aggregate\n",
    "result_df = df.groupby('subject_id').agg(\n",
    "    increase_flag=('increase_flag', 'max'),\n",
    "    start_time_of_increase=('start_time_of_increase', 'min')\n",
    ").reset_index()\n",
    "\n",
    "print(result_df[result_df['increase_flag']==0]['subject_id'].nunique()) #27111\n",
    "print(result_df[result_df['increase_flag']==1]['subject_id'].nunique()) #5269\n",
    "#Save the file\n",
    "result_df.to_csv(f'{base}48h_aki_pid_time_final.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcd7bef",
   "metadata": {},
   "source": [
    "### 7 day condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c5b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consider only patients with aki flag as 0, from previous 48h  criteria\n",
    "subject_ids7=result_df[result_df['increase_flag']==0]['subject_id'].unique()\n",
    "df_process=df.copy()\n",
    "df_process=df_process[df_process['subject_id'].isin(subject_ids7)]\n",
    "\n",
    "\n",
    "# Calculate baseline_scr and baseline_charttime\n",
    "baseline_info = df_process.groupby('subject_id').apply(lambda x: x.loc[x['valuenum'].idxmin()]).reset_index(drop=True)\n",
    "baseline_info = baseline_info[['subject_id', 'valuenum', 'charttime']]\n",
    "baseline_info.columns = ['subject_id', 'baseline_scr', 'baseline_charttime']\n",
    "df_process = pd.merge(df_process,baseline_info, on='subject_id', how='left')\n",
    "\n",
    "#Calculate flag and charttime\n",
    "df_process['increase_flag'] = 0\n",
    "df_process['start_time_of_increase'] = None\n",
    "\n",
    "for index, row in df_process.iterrows():\n",
    "    if row['charttime'] > row['baseline_charttime'] and row['charttime'] <= row['baseline_charttime'] + timedelta(days=7):\n",
    "        if row['valuenum'] >= 1.5 * row['baseline_scr']:\n",
    "            df_process.at[index, 'increase_flag'] = 1\n",
    "            df_process.at[index, 'start_time_of_increase'] = row['charttime']\n",
    "            \n",
    "#Save the file            \n",
    "df_process.to_csv(f'{base}7D_final.csv',index=False)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bde19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data \n",
    "#df_process[df_process['subject_id']==10119391]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300f9e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get onle one record per subject with flag and start time\n",
    "\n",
    "df_process['start_time_of_increase'] = pd.to_datetime(df_process['start_time_of_increase'])\n",
    "# Group by 'subject_id' and aggregate\n",
    "result_df = df_process.groupby('subject_id').agg(\n",
    "    increase_flag=('increase_flag', 'max'),\n",
    "    start_time_of_increase=('start_time_of_increase', 'min')\n",
    ").reset_index()\n",
    "\n",
    "#test \n",
    "#result_df[result_df['subject_id']==10131647]\n",
    "\n",
    "print(result_df[result_df['increase_flag']==1]['subject_id'].nunique()) #520\n",
    "print(result_df[result_df['increase_flag']==0]['subject_id'].nunique()) #21322\n",
    "\n",
    "#Save the file\n",
    "result_df.to_csv(f'{base}7D_aki_pid_time_final.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac01fe89",
   "metadata": {},
   "source": [
    "### Combine data from 48 hour and 7 day conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df71db96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files required for next steps :\n",
    "aki7=pd.read_csv(f'{base}7D_aki_pid_time_final.csv',index_col=0)\n",
    "aki7=aki7.reset_index()\n",
    "\n",
    "aki48=pd.read_csv(f'{base}48h_aki_pid_time_final.csv',index_col=0)\n",
    "aki48=aki48.reset_index()\n",
    "to_be_removed_from48=aki48[aki48['increase_flag']==0]['subject_id']\n",
    "\n",
    "aki48=aki48[~aki48['subject_id'].isin(to_be_removed_from48)] #retain only 1s in 48h\n",
    "\n",
    "\n",
    "#Concat\n",
    "aki_pid_time=pd.concat([aki48,aki7], ignore_index=True)\n",
    "\n",
    "\n",
    "#Since charttime was given as min start time, get 24 hour prior \n",
    "aki_pid_time['start_time_of_increase']=pd.to_datetime(aki_pid_time['start_time_of_increase'])\n",
    "#below code didn twork; so correct it when taking labs and medications\n",
    "aki_pid_time['min_time_required'] = aki_pid_time['start_time_of_increase'] - timedelta(hours=24) if pd.notnull(aki_pid_time['start_time_of_increase']).all() else aki_pid_time['start_time_of_increase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70a5cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge dataframe\n",
    "pat_adm1=pd.merge(pat_adm1,aki_pid_time,how='left',on='subject_id')\n",
    "\n",
    "# Check if 'increase flag' is 0 and 'min_time_required' is null\n",
    "# Assign 'dischtime' as 'min_time_required' for rows where the condition is true\n",
    "mask = (pat_adm1['increase_flag'] == 0.0) & pat_adm1['min_time_required'].isnull()\n",
    "pat_adm1.loc[mask, 'min_time_required'] = pat_adm1.loc[mask, 'dischtime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80ceafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the final file\n",
    "pat_adm1.to_csv(f'{base}pat_adm_aki_time_final.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4aabe5",
   "metadata": {},
   "source": [
    "### Final file - f'{base}pat_adm_aki_time_final.csv'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
